{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "lr=0.1 # learning rate 0.1\n",
    "test=False # train 인지 test 인지\n",
    "batch_size=128 # training 데이터셋 배치 사이즈\n",
    "batch_size_test=100 # test 데이터셋 배치 사이즈\n",
    "num_workers=4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset_train=CIFAR10(root='./data/', train=True, download=True, transform=transforms_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset_test = CIFAR10(root='./data', train=False, download=True, transform=transforms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size_test, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "classes = dataset_train.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (layer_2n): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_4n): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down_sample): IdentityPadding(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_6n): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (down_sample): IdentityPadding(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc_out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import resnet\n",
    "model = resnet()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print(f'train epoch: {epoch} [{batch_idx+1} / {len(train_loader)}] ')\n",
    "    print(f'loss: {train_loss / (batch_idx + 1)} | acc: {acc}')\n",
    "\n",
    "    return global_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, best_acc):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f'test epoch: {epoch} [{batch_idx+1} / {len(test_loader)}] ')\n",
    "    print(f'loss: {test_loss / (batch_idx + 1)} | acc: {acc}')\n",
    "\n",
    "    if (acc > best_acc):\n",
    "        best_acc = acc\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [391 / 391] \n",
      "loss: 1.890470641653251 | acc: 30.484\n",
      "test epoch: 1 [100 / 100] \n",
      "loss: 1.562194539308548 | acc: 42.29\n",
      "best test acc: 42.29\n",
      "train epoch: 2 [391 / 391] \n",
      "loss: 1.3909131876952814 | acc: 49.098\n",
      "test epoch: 2 [100 / 100] \n",
      "loss: 1.237586503624916 | acc: 55.34\n",
      "best test acc: 55.34\n",
      "train epoch: 3 [391 / 391] \n",
      "loss: 1.05346372289121 | acc: 62.558\n",
      "test epoch: 3 [100 / 100] \n",
      "loss: 0.9235042214393616 | acc: 67.65\n",
      "best test acc: 67.65\n",
      "train epoch: 4 [391 / 391] \n",
      "loss: 0.8406071752843345 | acc: 70.48\n",
      "test epoch: 4 [100 / 100] \n",
      "loss: 1.0228724491596222 | acc: 67.03\n",
      "best test acc: 67.65\n",
      "train epoch: 5 [391 / 391] \n",
      "loss: 0.7110887036451599 | acc: 75.284\n",
      "test epoch: 5 [100 / 100] \n",
      "loss: 0.8357703596353531 | acc: 72.81\n",
      "best test acc: 72.81\n",
      "train epoch: 6 [391 / 391] \n",
      "loss: 0.6311729858293558 | acc: 77.982\n",
      "test epoch: 6 [100 / 100] \n",
      "loss: 0.6497384083271026 | acc: 77.92\n",
      "best test acc: 77.92\n",
      "train epoch: 7 [391 / 391] \n",
      "loss: 0.5722666374405326 | acc: 80.216\n",
      "test epoch: 7 [100 / 100] \n",
      "loss: 0.8271482774615287 | acc: 73.56\n",
      "best test acc: 77.92\n",
      "train epoch: 8 [391 / 391] \n",
      "loss: 0.5332157007416191 | acc: 81.582\n",
      "test epoch: 8 [100 / 100] \n",
      "loss: 0.6337699124217033 | acc: 78.94\n",
      "best test acc: 78.94\n",
      "train epoch: 9 [391 / 391] \n",
      "loss: 0.5039925774192566 | acc: 82.538\n",
      "test epoch: 9 [100 / 100] \n",
      "loss: 0.6293070286512374 | acc: 79.19\n",
      "best test acc: 79.19\n",
      "train epoch: 10 [391 / 391] \n",
      "loss: 0.4733144432077627 | acc: 83.822\n",
      "test epoch: 10 [100 / 100] \n",
      "loss: 0.685758028626442 | acc: 77.5\n",
      "best test acc: 79.19\n",
      "train epoch: 11 [391 / 391] \n",
      "loss: 0.45220570552074696 | acc: 84.436\n",
      "test epoch: 11 [100 / 100] \n",
      "loss: 0.5649736049771309 | acc: 81.58\n",
      "best test acc: 81.58\n",
      "train epoch: 12 [391 / 391] \n",
      "loss: 0.4333844972617181 | acc: 84.9\n",
      "test epoch: 12 [100 / 100] \n",
      "loss: 0.5041000843048096 | acc: 83.08\n",
      "best test acc: 83.08\n",
      "train epoch: 13 [391 / 391] \n",
      "loss: 0.41228190033941925 | acc: 85.724\n",
      "test epoch: 13 [100 / 100] \n",
      "loss: 0.5315345188975334 | acc: 81.88\n",
      "best test acc: 83.08\n",
      "train epoch: 14 [391 / 391] \n",
      "loss: 0.3976858400399118 | acc: 86.206\n",
      "test epoch: 14 [100 / 100] \n",
      "loss: 0.49962988451123236 | acc: 83.18\n",
      "best test acc: 83.18\n",
      "train epoch: 15 [391 / 391] \n",
      "loss: 0.38192280204704654 | acc: 86.776\n",
      "test epoch: 15 [100 / 100] \n",
      "loss: 0.5471616734564304 | acc: 82.37\n",
      "best test acc: 83.18\n",
      "train epoch: 16 [391 / 391] \n",
      "loss: 0.37380905411280024 | acc: 87.07\n",
      "test epoch: 16 [100 / 100] \n",
      "loss: 0.5115538278222084 | acc: 83.26\n",
      "best test acc: 83.26\n",
      "train epoch: 17 [391 / 391] \n",
      "loss: 0.35714128182824617 | acc: 87.574\n",
      "test epoch: 17 [100 / 100] \n",
      "loss: 0.4951937878131866 | acc: 84.0\n",
      "best test acc: 84.0\n",
      "train epoch: 18 [391 / 391] \n",
      "loss: 0.35019070687501325 | acc: 87.79\n",
      "test epoch: 18 [100 / 100] \n",
      "loss: 0.45073688447475435 | acc: 85.2\n",
      "best test acc: 85.2\n",
      "train epoch: 19 [391 / 391] \n",
      "loss: 0.3458528769824206 | acc: 87.994\n",
      "test epoch: 19 [100 / 100] \n",
      "loss: 0.4629858139157295 | acc: 85.09\n",
      "best test acc: 85.2\n",
      "train epoch: 20 [391 / 391] \n",
      "loss: 0.33242766147531816 | acc: 88.408\n",
      "test epoch: 20 [100 / 100] \n",
      "loss: 0.5177225959300995 | acc: 84.02\n",
      "best test acc: 85.2\n",
      "train epoch: 21 [391 / 391] \n",
      "loss: 0.3278069112764295 | acc: 88.506\n",
      "test epoch: 21 [100 / 100] \n",
      "loss: 0.4598868055641651 | acc: 84.82\n",
      "best test acc: 85.2\n",
      "train epoch: 22 [391 / 391] \n",
      "loss: 0.31074631892506727 | acc: 89.34\n",
      "test epoch: 22 [100 / 100] \n",
      "loss: 0.575338482260704 | acc: 82.48\n",
      "best test acc: 85.2\n",
      "train epoch: 23 [391 / 391] \n",
      "loss: 0.31376482298611985 | acc: 89.116\n",
      "test epoch: 23 [100 / 100] \n",
      "loss: 0.432685699313879 | acc: 86.53\n",
      "best test acc: 86.53\n",
      "train epoch: 24 [391 / 391] \n",
      "loss: 0.3041282914712301 | acc: 89.394\n",
      "test epoch: 24 [100 / 100] \n",
      "loss: 0.46867884665727616 | acc: 85.26\n",
      "best test acc: 86.53\n",
      "train epoch: 25 [391 / 391] \n",
      "loss: 0.3011189581792983 | acc: 89.418\n",
      "test epoch: 25 [100 / 100] \n",
      "loss: 0.3885942651331425 | acc: 86.92\n",
      "best test acc: 86.92\n",
      "train epoch: 26 [391 / 391] \n",
      "loss: 0.2954676991990765 | acc: 89.69\n",
      "test epoch: 26 [100 / 100] \n",
      "loss: 0.4314654752612114 | acc: 85.95\n",
      "best test acc: 86.92\n",
      "train epoch: 27 [391 / 391] \n",
      "loss: 0.2870543129608759 | acc: 90.074\n",
      "test epoch: 27 [100 / 100] \n",
      "loss: 0.4579788833856583 | acc: 85.49\n",
      "best test acc: 86.92\n",
      "train epoch: 28 [391 / 391] \n",
      "loss: 0.28195830099189373 | acc: 90.29\n",
      "test epoch: 28 [100 / 100] \n",
      "loss: 0.5212496218085289 | acc: 83.66\n",
      "best test acc: 86.92\n",
      "train epoch: 29 [391 / 391] \n",
      "loss: 0.2796489105123998 | acc: 90.228\n",
      "test epoch: 29 [100 / 100] \n",
      "loss: 0.5069488847255706 | acc: 84.55\n",
      "best test acc: 86.92\n",
      "train epoch: 30 [391 / 391] \n",
      "loss: 0.2743819469152509 | acc: 90.562\n",
      "test epoch: 30 [100 / 100] \n",
      "loss: 0.5736686733365058 | acc: 82.53\n",
      "best test acc: 86.92\n",
      "train epoch: 31 [391 / 391] \n",
      "loss: 0.2683260740755159 | acc: 90.742\n",
      "test epoch: 31 [100 / 100] \n",
      "loss: 0.5644783303141594 | acc: 82.92\n",
      "best test acc: 86.92\n",
      "train epoch: 32 [391 / 391] \n",
      "loss: 0.2673830079757954 | acc: 90.602\n",
      "test epoch: 32 [100 / 100] \n",
      "loss: 0.6823180395364762 | acc: 80.68\n",
      "best test acc: 86.92\n",
      "train epoch: 33 [391 / 391] \n",
      "loss: 0.2633508456599377 | acc: 90.814\n",
      "test epoch: 33 [100 / 100] \n",
      "loss: 0.4871442022919655 | acc: 84.66\n",
      "best test acc: 86.92\n",
      "train epoch: 34 [391 / 391] \n",
      "loss: 0.2623970827368824 | acc: 90.666\n",
      "test epoch: 34 [100 / 100] \n",
      "loss: 0.48395109429955485 | acc: 85.04\n",
      "best test acc: 86.92\n",
      "train epoch: 35 [391 / 391] \n",
      "loss: 0.2617029285491885 | acc: 90.8\n",
      "test epoch: 35 [100 / 100] \n",
      "loss: 0.47510319486260416 | acc: 85.91\n",
      "best test acc: 86.92\n",
      "train epoch: 36 [391 / 391] \n",
      "loss: 0.25255569626989266 | acc: 91.092\n",
      "test epoch: 36 [100 / 100] \n",
      "loss: 0.452065013051033 | acc: 85.32\n",
      "best test acc: 86.92\n",
      "train epoch: 37 [391 / 391] \n",
      "loss: 0.25248211058204434 | acc: 91.196\n",
      "test epoch: 37 [100 / 100] \n",
      "loss: 0.41944953203201296 | acc: 86.8\n",
      "best test acc: 86.92\n",
      "train epoch: 38 [391 / 391] \n",
      "loss: 0.2494870395306736 | acc: 91.344\n",
      "test epoch: 38 [100 / 100] \n",
      "loss: 0.43782832235097885 | acc: 86.03\n",
      "best test acc: 86.92\n",
      "train epoch: 39 [391 / 391] \n",
      "loss: 0.24446297374070453 | acc: 91.49\n",
      "test epoch: 39 [100 / 100] \n",
      "loss: 0.45402466878294945 | acc: 85.9\n",
      "best test acc: 86.92\n",
      "train epoch: 40 [391 / 391] \n",
      "loss: 0.2440582351077853 | acc: 91.386\n",
      "test epoch: 40 [100 / 100] \n",
      "loss: 0.6598299944400787 | acc: 81.52\n",
      "best test acc: 86.92\n",
      "train epoch: 41 [391 / 391] \n",
      "loss: 0.24338224776032025 | acc: 91.466\n",
      "test epoch: 41 [100 / 100] \n",
      "loss: 0.50546710729599 | acc: 84.38\n",
      "best test acc: 86.92\n",
      "train epoch: 42 [391 / 391] \n",
      "loss: 0.2413610596486065 | acc: 91.624\n",
      "test epoch: 42 [100 / 100] \n",
      "loss: 0.42025838121771814 | acc: 86.85\n",
      "best test acc: 86.92\n",
      "train epoch: 43 [391 / 391] \n",
      "loss: 0.23254069045681477 | acc: 91.754\n",
      "test epoch: 43 [100 / 100] \n",
      "loss: 0.42226595506072045 | acc: 87.12\n",
      "best test acc: 87.12\n",
      "train epoch: 44 [391 / 391] \n",
      "loss: 0.23843778786070816 | acc: 91.558\n",
      "test epoch: 44 [100 / 100] \n",
      "loss: 0.4122452083230019 | acc: 86.78\n",
      "best test acc: 87.12\n",
      "train epoch: 45 [391 / 391] \n",
      "loss: 0.2287632772684707 | acc: 92.036\n",
      "test epoch: 45 [100 / 100] \n",
      "loss: 0.43199418812990187 | acc: 86.05\n",
      "best test acc: 87.12\n",
      "train epoch: 46 [391 / 391] \n",
      "loss: 0.23404205636218992 | acc: 91.84\n",
      "test epoch: 46 [100 / 100] \n",
      "loss: 0.40211165085434913 | acc: 87.63\n",
      "best test acc: 87.63\n",
      "train epoch: 47 [391 / 391] \n",
      "loss: 0.22818968929064548 | acc: 92.11\n",
      "test epoch: 47 [100 / 100] \n",
      "loss: 0.4110893365740776 | acc: 87.35\n",
      "best test acc: 87.63\n",
      "train epoch: 48 [391 / 391] \n",
      "loss: 0.22786500264921455 | acc: 92.008\n",
      "test epoch: 48 [100 / 100] \n",
      "loss: 0.43008665487170217 | acc: 86.65\n",
      "best test acc: 87.63\n",
      "train epoch: 49 [391 / 391] \n",
      "loss: 0.22515318209253005 | acc: 91.976\n",
      "test epoch: 49 [100 / 100] \n",
      "loss: 0.4280593632161617 | acc: 86.52\n",
      "best test acc: 87.63\n",
      "train epoch: 50 [391 / 391] \n",
      "loss: 0.22087741117267048 | acc: 92.178\n",
      "test epoch: 50 [100 / 100] \n",
      "loss: 0.4116308061778545 | acc: 87.24\n",
      "best test acc: 87.63\n",
      "train epoch: 51 [391 / 391] \n",
      "loss: 0.22302319048463232 | acc: 92.156\n",
      "test epoch: 51 [100 / 100] \n",
      "loss: 0.6657842674851417 | acc: 81.33\n",
      "best test acc: 87.63\n",
      "train epoch: 52 [391 / 391] \n",
      "loss: 0.21884348424499298 | acc: 92.276\n",
      "test epoch: 52 [100 / 100] \n",
      "loss: 0.4682171097397804 | acc: 86.71\n",
      "best test acc: 87.63\n",
      "train epoch: 53 [391 / 391] \n",
      "loss: 0.22196627076705702 | acc: 92.252\n",
      "test epoch: 53 [100 / 100] \n",
      "loss: 0.4373793312907219 | acc: 86.21\n",
      "best test acc: 87.63\n",
      "train epoch: 54 [391 / 391] \n",
      "loss: 0.21996520665448036 | acc: 92.352\n",
      "test epoch: 54 [100 / 100] \n",
      "loss: 0.5358280582726002 | acc: 84.1\n",
      "best test acc: 87.63\n",
      "train epoch: 55 [391 / 391] \n",
      "loss: 0.21038341184939874 | acc: 92.668\n",
      "test epoch: 55 [100 / 100] \n",
      "loss: 0.3868496190011501 | acc: 87.93\n",
      "best test acc: 87.93\n",
      "train epoch: 56 [391 / 391] \n",
      "loss: 0.2186437718131963 | acc: 92.358\n",
      "test epoch: 56 [100 / 100] \n",
      "loss: 0.4833337685465813 | acc: 85.97\n",
      "best test acc: 87.93\n",
      "train epoch: 57 [391 / 391] \n",
      "loss: 0.21796449944567498 | acc: 92.29\n",
      "test epoch: 57 [100 / 100] \n",
      "loss: 0.38574197605252264 | acc: 87.79\n",
      "best test acc: 87.93\n",
      "train epoch: 58 [391 / 391] \n",
      "loss: 0.20996601387972721 | acc: 92.678\n",
      "test epoch: 58 [100 / 100] \n",
      "loss: 0.38695491760969164 | acc: 87.58\n",
      "best test acc: 87.93\n",
      "train epoch: 59 [391 / 391] \n",
      "loss: 0.2096078713112475 | acc: 92.482\n",
      "test epoch: 59 [100 / 100] \n",
      "loss: 0.4061898536980152 | acc: 87.54\n",
      "best test acc: 87.93\n",
      "train epoch: 60 [391 / 391] \n",
      "loss: 0.2063371546928535 | acc: 92.83\n",
      "test epoch: 60 [100 / 100] \n",
      "loss: 0.5045597577095031 | acc: 84.74\n",
      "best test acc: 87.93\n",
      "train epoch: 61 [391 / 391] \n",
      "loss: 0.21585220115645157 | acc: 92.292\n",
      "test epoch: 61 [100 / 100] \n",
      "loss: 0.4343254396319389 | acc: 87.09\n",
      "best test acc: 87.93\n",
      "train epoch: 62 [391 / 391] \n",
      "loss: 0.2084241028579757 | acc: 92.672\n",
      "test epoch: 62 [100 / 100] \n",
      "loss: 0.392007717192173 | acc: 87.99\n",
      "best test acc: 87.99\n",
      "train epoch: 63 [391 / 391] \n",
      "loss: 0.20459797886936257 | acc: 92.834\n",
      "test epoch: 63 [100 / 100] \n",
      "loss: 0.5215398187935353 | acc: 84.94\n",
      "best test acc: 87.99\n",
      "train epoch: 64 [391 / 391] \n",
      "loss: 0.20391603284860815 | acc: 92.712\n",
      "test epoch: 64 [100 / 100] \n",
      "loss: 0.432546104490757 | acc: 86.86\n",
      "best test acc: 87.99\n",
      "train epoch: 65 [391 / 391] \n",
      "loss: 0.2041144352930281 | acc: 92.794\n",
      "test epoch: 65 [100 / 100] \n",
      "loss: 0.43598401963710787 | acc: 87.25\n",
      "best test acc: 87.99\n",
      "train epoch: 66 [391 / 391] \n",
      "loss: 0.20470069122055304 | acc: 92.806\n",
      "test epoch: 66 [100 / 100] \n",
      "loss: 0.4102123834192753 | acc: 87.15\n",
      "best test acc: 87.99\n",
      "train epoch: 67 [391 / 391] \n",
      "loss: 0.19527984395280212 | acc: 93.196\n",
      "test epoch: 67 [100 / 100] \n",
      "loss: 0.6255926832556724 | acc: 83.44\n",
      "best test acc: 87.99\n",
      "train epoch: 68 [391 / 391] \n",
      "loss: 0.2014827004745793 | acc: 92.926\n",
      "test epoch: 68 [100 / 100] \n",
      "loss: 0.43309017837047575 | acc: 86.68\n",
      "best test acc: 87.99\n",
      "train epoch: 69 [391 / 391] \n",
      "loss: 0.20690436386848654 | acc: 92.582\n",
      "test epoch: 69 [100 / 100] \n",
      "loss: 0.382844617664814 | acc: 88.12\n",
      "best test acc: 88.12\n",
      "train epoch: 70 [391 / 391] \n",
      "loss: 0.20093343576506886 | acc: 92.974\n",
      "test epoch: 70 [100 / 100] \n",
      "loss: 0.36914208412170413 | acc: 88.35\n",
      "best test acc: 88.35\n",
      "train epoch: 71 [391 / 391] \n",
      "loss: 0.1907680429842161 | acc: 93.308\n",
      "test epoch: 71 [100 / 100] \n",
      "loss: 0.4511375761032104 | acc: 87.02\n",
      "best test acc: 88.35\n",
      "train epoch: 72 [391 / 391] \n",
      "loss: 0.20625846202263748 | acc: 92.824\n",
      "test epoch: 72 [100 / 100] \n",
      "loss: 0.4153064577281475 | acc: 87.88\n",
      "best test acc: 88.35\n",
      "train epoch: 73 [391 / 391] \n",
      "loss: 0.19116573045244606 | acc: 93.296\n",
      "test epoch: 73 [100 / 100] \n",
      "loss: 0.46706986770033837 | acc: 86.69\n",
      "best test acc: 88.35\n",
      "train epoch: 74 [391 / 391] \n",
      "loss: 0.19419754321312965 | acc: 93.142\n",
      "test epoch: 74 [100 / 100] \n",
      "loss: 0.55049803763628 | acc: 84.56\n",
      "best test acc: 88.35\n",
      "train epoch: 75 [391 / 391] \n",
      "loss: 0.19775074537452833 | acc: 93.148\n",
      "test epoch: 75 [100 / 100] \n",
      "loss: 0.6014049604535103 | acc: 82.77\n",
      "best test acc: 88.35\n",
      "train epoch: 76 [391 / 391] \n",
      "loss: 0.1982612132149584 | acc: 93.138\n",
      "test epoch: 76 [100 / 100] \n",
      "loss: 0.3880618372559547 | acc: 88.2\n",
      "best test acc: 88.35\n",
      "train epoch: 77 [391 / 391] \n",
      "loss: 0.1925894007505968 | acc: 93.228\n",
      "test epoch: 77 [100 / 100] \n",
      "loss: 0.39015659540891645 | acc: 88.22\n",
      "best test acc: 88.35\n",
      "train epoch: 78 [391 / 391] \n",
      "loss: 0.19331431068727734 | acc: 93.18\n",
      "test epoch: 78 [100 / 100] \n",
      "loss: 0.3949608636647463 | acc: 88.46\n",
      "best test acc: 88.46\n",
      "train epoch: 79 [391 / 391] \n",
      "loss: 0.19360481200696866 | acc: 93.13\n",
      "test epoch: 79 [100 / 100] \n",
      "loss: 0.48949352025985715 | acc: 85.67\n",
      "best test acc: 88.46\n",
      "train epoch: 80 [391 / 391] \n",
      "loss: 0.194017207912167 | acc: 93.186\n",
      "test epoch: 80 [100 / 100] \n",
      "loss: 0.49072945177555083 | acc: 85.79\n",
      "best test acc: 88.46\n",
      "train epoch: 81 [391 / 391] \n",
      "loss: 0.1898560999985546 | acc: 93.35\n",
      "test epoch: 81 [100 / 100] \n",
      "loss: 0.4648884907364845 | acc: 86.65\n",
      "best test acc: 88.46\n",
      "train epoch: 82 [391 / 391] \n",
      "loss: 0.19348417604556473 | acc: 93.232\n",
      "test epoch: 82 [100 / 100] \n",
      "loss: 0.3763535811007023 | acc: 88.62\n",
      "best test acc: 88.62\n",
      "train epoch: 83 [391 / 391] \n",
      "loss: 0.1908192656114888 | acc: 93.328\n",
      "test epoch: 83 [100 / 100] \n",
      "loss: 0.4517508046329021 | acc: 86.89\n",
      "best test acc: 88.62\n",
      "train epoch: 84 [391 / 391] \n",
      "loss: 0.19147001991948814 | acc: 93.328\n",
      "test epoch: 84 [100 / 100] \n",
      "loss: 0.4212146598100662 | acc: 87.66\n",
      "best test acc: 88.62\n",
      "train epoch: 85 [391 / 391] \n",
      "loss: 0.1870748338568241 | acc: 93.396\n",
      "test epoch: 85 [100 / 100] \n",
      "loss: 0.40307479843497274 | acc: 88.39\n",
      "best test acc: 88.62\n",
      "train epoch: 86 [391 / 391] \n",
      "loss: 0.19478367572016728 | acc: 93.056\n",
      "test epoch: 86 [100 / 100] \n",
      "loss: 0.43933322221040727 | acc: 87.11\n",
      "best test acc: 88.62\n",
      "train epoch: 87 [391 / 391] \n",
      "loss: 0.19112322826290984 | acc: 93.26\n",
      "test epoch: 87 [100 / 100] \n",
      "loss: 0.544820639193058 | acc: 84.44\n",
      "best test acc: 88.62\n",
      "train epoch: 88 [391 / 391] \n",
      "loss: 0.18668454797828898 | acc: 93.492\n",
      "test epoch: 88 [100 / 100] \n",
      "loss: 0.4468948508799076 | acc: 86.76\n",
      "best test acc: 88.62\n",
      "train epoch: 89 [391 / 391] \n",
      "loss: 0.18573921937924212 | acc: 93.57\n",
      "test epoch: 89 [100 / 100] \n",
      "loss: 0.43691312074661254 | acc: 86.93\n",
      "best test acc: 88.62\n",
      "train epoch: 90 [391 / 391] \n",
      "loss: 0.1833221357782631 | acc: 93.564\n",
      "test epoch: 90 [100 / 100] \n",
      "loss: 0.46846939146518707 | acc: 85.97\n",
      "best test acc: 88.62\n",
      "train epoch: 91 [391 / 391] \n",
      "loss: 0.18933437494060879 | acc: 93.206\n",
      "test epoch: 91 [100 / 100] \n",
      "loss: 0.42799366034567354 | acc: 87.05\n",
      "best test acc: 88.62\n",
      "train epoch: 92 [391 / 391] \n",
      "loss: 0.18834142291637332 | acc: 93.44\n",
      "test epoch: 92 [100 / 100] \n",
      "loss: 0.3972996063530445 | acc: 88.1\n",
      "best test acc: 88.62\n",
      "train epoch: 93 [391 / 391] \n",
      "loss: 0.186005138403848 | acc: 93.378\n",
      "test epoch: 93 [100 / 100] \n",
      "loss: 0.4292824037373066 | acc: 87.87\n",
      "best test acc: 88.62\n",
      "train epoch: 94 [391 / 391] \n",
      "loss: 0.18296838714681624 | acc: 93.506\n",
      "test epoch: 94 [100 / 100] \n",
      "loss: 0.416361386179924 | acc: 87.75\n",
      "best test acc: 88.62\n",
      "train epoch: 95 [391 / 391] \n",
      "loss: 0.18382493002564096 | acc: 93.564\n",
      "test epoch: 95 [100 / 100] \n",
      "loss: 0.40696114145219325 | acc: 88.16\n",
      "best test acc: 88.62\n",
      "train epoch: 96 [391 / 391] \n",
      "loss: 0.18231815027306453 | acc: 93.644\n",
      "test epoch: 96 [100 / 100] \n",
      "loss: 0.4054722212255001 | acc: 88.02\n",
      "best test acc: 88.62\n",
      "train epoch: 97 [391 / 391] \n",
      "loss: 0.18741794191586697 | acc: 93.482\n",
      "test epoch: 97 [100 / 100] \n",
      "loss: 0.426612860262394 | acc: 87.71\n",
      "best test acc: 88.62\n",
      "train epoch: 98 [391 / 391] \n",
      "loss: 0.18319213660934086 | acc: 93.514\n",
      "test epoch: 98 [100 / 100] \n",
      "loss: 0.39358835369348527 | acc: 88.33\n",
      "best test acc: 88.62\n",
      "train epoch: 99 [391 / 391] \n",
      "loss: 0.18193682298407227 | acc: 93.628\n",
      "test epoch: 99 [100 / 100] \n",
      "loss: 0.4470694248378277 | acc: 86.25\n",
      "best test acc: 88.62\n",
      "train epoch: 100 [391 / 391] \n",
      "loss: 0.1812162794687254 | acc: 93.558\n",
      "test epoch: 100 [100 / 100] \n",
      "loss: 0.40371268942952154 | acc: 87.74\n",
      "best test acc: 88.62\n",
      "train epoch: 101 [391 / 391] \n",
      "loss: 0.18428799921594313 | acc: 93.578\n",
      "test epoch: 101 [100 / 100] \n",
      "loss: 0.3964353282749653 | acc: 87.91\n",
      "best test acc: 88.62\n",
      "train epoch: 102 [391 / 391] \n",
      "loss: 0.18260519578100165 | acc: 93.554\n",
      "test epoch: 102 [100 / 100] \n",
      "loss: 0.5296565710008144 | acc: 84.16\n",
      "best test acc: 88.62\n",
      "train epoch: 103 [391 / 391] \n",
      "loss: 0.1779549729431529 | acc: 93.722\n",
      "test epoch: 103 [100 / 100] \n",
      "loss: 0.5644827038049698 | acc: 84.34\n",
      "best test acc: 88.62\n",
      "train epoch: 104 [391 / 391] \n",
      "loss: 0.18020187592719827 | acc: 93.636\n",
      "test epoch: 104 [100 / 100] \n",
      "loss: 0.43818767338991166 | acc: 87.65\n",
      "best test acc: 88.62\n",
      "train epoch: 105 [391 / 391] \n",
      "loss: 0.18599275568180987 | acc: 93.366\n",
      "test epoch: 105 [100 / 100] \n",
      "loss: 0.36332196414470674 | acc: 88.71\n",
      "best test acc: 88.71\n",
      "train epoch: 106 [391 / 391] \n",
      "loss: 0.1752827720587973 | acc: 93.816\n",
      "test epoch: 106 [100 / 100] \n",
      "loss: 0.43125358477234843 | acc: 87.0\n",
      "best test acc: 88.71\n",
      "train epoch: 107 [391 / 391] \n",
      "loss: 0.18299692760571798 | acc: 93.524\n",
      "test epoch: 107 [100 / 100] \n",
      "loss: 0.4471159188449383 | acc: 87.41\n",
      "best test acc: 88.71\n",
      "train epoch: 108 [391 / 391] \n",
      "loss: 0.1784536658269365 | acc: 93.728\n",
      "test epoch: 108 [100 / 100] \n",
      "loss: 0.4683400604128838 | acc: 86.41\n",
      "best test acc: 88.71\n",
      "train epoch: 109 [391 / 391] \n",
      "loss: 0.18013899788603455 | acc: 93.486\n",
      "test epoch: 109 [100 / 100] \n",
      "loss: 0.6029872472584248 | acc: 83.2\n",
      "best test acc: 88.71\n",
      "train epoch: 110 [391 / 391] \n",
      "loss: 0.1798518894483214 | acc: 93.734\n",
      "test epoch: 110 [100 / 100] \n",
      "loss: 0.4020792844891548 | acc: 87.49\n",
      "best test acc: 88.71\n",
      "train epoch: 111 [391 / 391] \n",
      "loss: 0.1760801669886655 | acc: 93.772\n",
      "test epoch: 111 [100 / 100] \n",
      "loss: 0.43682252272963523 | acc: 86.99\n",
      "best test acc: 88.71\n",
      "train epoch: 112 [391 / 391] \n",
      "loss: 0.17984486736185715 | acc: 93.744\n",
      "test epoch: 112 [100 / 100] \n",
      "loss: 0.4152724266052246 | acc: 88.37\n",
      "best test acc: 88.71\n",
      "train epoch: 113 [391 / 391] \n",
      "loss: 0.18098008861322232 | acc: 93.656\n",
      "test epoch: 113 [100 / 100] \n",
      "loss: 0.3948114866018295 | acc: 88.71\n",
      "best test acc: 88.71\n",
      "train epoch: 114 [391 / 391] \n",
      "loss: 0.17357404622466058 | acc: 93.836\n",
      "test epoch: 114 [100 / 100] \n",
      "loss: 0.4808481338620186 | acc: 85.71\n",
      "best test acc: 88.71\n",
      "train epoch: 115 [391 / 391] \n",
      "loss: 0.18240294946581506 | acc: 93.63\n",
      "test epoch: 115 [100 / 100] \n",
      "loss: 0.4401908949762583 | acc: 87.89\n",
      "best test acc: 88.71\n",
      "train epoch: 116 [391 / 391] \n",
      "loss: 0.17446929827103835 | acc: 93.89\n",
      "test epoch: 116 [100 / 100] \n",
      "loss: 0.4262120547890663 | acc: 87.21\n",
      "best test acc: 88.71\n",
      "train epoch: 117 [391 / 391] \n",
      "loss: 0.1726525070436318 | acc: 93.888\n",
      "test epoch: 117 [100 / 100] \n",
      "loss: 0.49143689781427385 | acc: 86.58\n",
      "best test acc: 88.71\n",
      "train epoch: 118 [391 / 391] \n",
      "loss: 0.17433031220608355 | acc: 93.912\n",
      "test epoch: 118 [100 / 100] \n",
      "loss: 0.43716231897473334 | acc: 87.75\n",
      "best test acc: 88.71\n",
      "train epoch: 119 [391 / 391] \n",
      "loss: 0.17790737604279347 | acc: 93.698\n",
      "test epoch: 119 [100 / 100] \n",
      "loss: 0.4433045732975006 | acc: 86.8\n",
      "best test acc: 88.71\n",
      "train epoch: 120 [391 / 391] \n",
      "loss: 0.17168337567840392 | acc: 93.912\n",
      "test epoch: 120 [100 / 100] \n",
      "loss: 0.4617005920410156 | acc: 86.94\n",
      "best test acc: 88.71\n",
      "train epoch: 121 [391 / 391] \n",
      "loss: 0.17313823995687772 | acc: 93.952\n",
      "test epoch: 121 [100 / 100] \n",
      "loss: 0.41296830907464027 | acc: 87.31\n",
      "best test acc: 88.71\n",
      "train epoch: 122 [391 / 391] \n",
      "loss: 0.17303718277789137 | acc: 93.868\n",
      "test epoch: 122 [100 / 100] \n",
      "loss: 0.46239665895700455 | acc: 86.54\n",
      "best test acc: 88.71\n",
      "train epoch: 123 [391 / 391] \n",
      "loss: 0.17033326487673822 | acc: 94.008\n",
      "test epoch: 123 [100 / 100] \n",
      "loss: 0.414702804684639 | acc: 87.61\n",
      "best test acc: 88.71\n",
      "train epoch: 124 [391 / 391] \n",
      "loss: 0.1675731706554475 | acc: 94.046\n",
      "test epoch: 124 [100 / 100] \n",
      "loss: 0.4617915891110897 | acc: 87.39\n",
      "best test acc: 88.71\n",
      "train epoch: 125 [391 / 391] \n",
      "loss: 0.18072119695336922 | acc: 93.718\n",
      "test epoch: 125 [100 / 100] \n",
      "loss: 0.3885712371766567 | acc: 87.75\n",
      "best test acc: 88.71\n",
      "train epoch: 126 [391 / 391] \n",
      "loss: 0.1709364564122294 | acc: 93.9\n",
      "test epoch: 126 [100 / 100] \n",
      "loss: 0.49249878376722334 | acc: 86.24\n",
      "best test acc: 88.71\n",
      "train epoch: 127 [391 / 391] \n",
      "loss: 0.17488608219663201 | acc: 93.816\n",
      "test epoch: 127 [100 / 100] \n",
      "loss: 0.38042382672429087 | acc: 88.58\n",
      "best test acc: 88.71\n",
      "train epoch: 128 [391 / 391] \n",
      "loss: 0.17436508521856858 | acc: 93.77\n",
      "test epoch: 128 [100 / 100] \n",
      "loss: 0.48563034042716025 | acc: 86.99\n",
      "best test acc: 88.71\n",
      "train epoch: 129 [391 / 391] \n",
      "loss: 0.16887095679178873 | acc: 94.072\n",
      "test epoch: 129 [100 / 100] \n",
      "loss: 0.4364941485226154 | acc: 87.04\n",
      "best test acc: 88.71\n",
      "train epoch: 130 [391 / 391] \n",
      "loss: 0.17385770074661125 | acc: 93.85\n",
      "test epoch: 130 [100 / 100] \n",
      "loss: 0.4630325318872929 | acc: 86.39\n",
      "best test acc: 88.71\n",
      "train epoch: 131 [391 / 391] \n",
      "loss: 0.1712769425338339 | acc: 93.966\n",
      "test epoch: 131 [100 / 100] \n",
      "loss: 0.5239677155017852 | acc: 85.56\n",
      "best test acc: 88.71\n",
      "train epoch: 132 [391 / 391] \n",
      "loss: 0.1683920379394613 | acc: 93.988\n",
      "test epoch: 132 [100 / 100] \n",
      "loss: 0.41484856978058815 | acc: 87.71\n",
      "best test acc: 88.71\n",
      "train epoch: 133 [391 / 391] \n",
      "loss: 0.1719088773135944 | acc: 93.934\n",
      "test epoch: 133 [100 / 100] \n",
      "loss: 0.4139766305685043 | acc: 87.79\n",
      "best test acc: 88.71\n",
      "train epoch: 134 [391 / 391] \n",
      "loss: 0.17128665162169415 | acc: 93.918\n",
      "test epoch: 134 [100 / 100] \n",
      "loss: 0.4509768567979336 | acc: 86.75\n",
      "best test acc: 88.71\n",
      "train epoch: 135 [391 / 391] \n",
      "loss: 0.17180440363371768 | acc: 93.954\n",
      "test epoch: 135 [100 / 100] \n",
      "loss: 0.4515910491347313 | acc: 87.13\n",
      "best test acc: 88.71\n",
      "train epoch: 136 [391 / 391] \n",
      "loss: 0.17217894393922117 | acc: 94.028\n",
      "test epoch: 136 [100 / 100] \n",
      "loss: 0.41603398278355597 | acc: 87.53\n",
      "best test acc: 88.71\n",
      "train epoch: 137 [391 / 391] \n",
      "loss: 0.16891387348894574 | acc: 94.196\n",
      "test epoch: 137 [100 / 100] \n",
      "loss: 0.4706656487286091 | acc: 86.66\n",
      "best test acc: 88.71\n",
      "train epoch: 138 [391 / 391] \n",
      "loss: 0.1689082948047944 | acc: 94.068\n",
      "test epoch: 138 [100 / 100] \n",
      "loss: 0.42939953625202176 | acc: 87.83\n",
      "best test acc: 88.71\n",
      "train epoch: 139 [391 / 391] \n",
      "loss: 0.17209513322509767 | acc: 93.96\n",
      "test epoch: 139 [100 / 100] \n",
      "loss: 0.43693847104907035 | acc: 87.12\n",
      "best test acc: 88.71\n",
      "train epoch: 140 [391 / 391] \n",
      "loss: 0.1677277415342953 | acc: 94.1\n",
      "test epoch: 140 [100 / 100] \n",
      "loss: 0.4142476451396942 | acc: 87.54\n",
      "best test acc: 88.71\n",
      "train epoch: 141 [391 / 391] \n",
      "loss: 0.1719514041224404 | acc: 94.008\n",
      "test epoch: 141 [100 / 100] \n",
      "loss: 0.43253798872232435 | acc: 87.22\n",
      "best test acc: 88.71\n",
      "train epoch: 142 [391 / 391] \n",
      "loss: 0.1681177755889228 | acc: 94.074\n",
      "test epoch: 142 [100 / 100] \n",
      "loss: 0.49872836977243423 | acc: 86.42\n",
      "best test acc: 88.71\n",
      "train epoch: 143 [391 / 391] \n",
      "loss: 0.17042674719715667 | acc: 94.076\n",
      "test epoch: 143 [100 / 100] \n",
      "loss: 0.3905482617765665 | acc: 88.31\n",
      "best test acc: 88.71\n",
      "train epoch: 144 [391 / 391] \n",
      "loss: 0.16833425870598734 | acc: 93.994\n",
      "test epoch: 144 [100 / 100] \n",
      "loss: 0.4923349802196026 | acc: 86.67\n",
      "best test acc: 88.71\n",
      "train epoch: 145 [391 / 391] \n",
      "loss: 0.17120447447118553 | acc: 93.986\n",
      "test epoch: 145 [100 / 100] \n",
      "loss: 0.46419750407338145 | acc: 86.48\n",
      "best test acc: 88.71\n",
      "train epoch: 146 [391 / 391] \n",
      "loss: 0.17084852827098362 | acc: 94.052\n",
      "test epoch: 146 [100 / 100] \n",
      "loss: 0.3921194742619991 | acc: 88.19\n",
      "best test acc: 88.71\n",
      "train epoch: 147 [391 / 391] \n",
      "loss: 0.16380760737735292 | acc: 94.108\n",
      "test epoch: 147 [100 / 100] \n",
      "loss: 0.48941812589764594 | acc: 86.7\n",
      "best test acc: 88.71\n",
      "train epoch: 148 [391 / 391] \n",
      "loss: 0.17024515350075328 | acc: 93.968\n",
      "test epoch: 148 [100 / 100] \n",
      "loss: 0.4439864084124565 | acc: 87.12\n",
      "best test acc: 88.71\n",
      "train epoch: 149 [391 / 391] \n",
      "loss: 0.1689635763597458 | acc: 94.048\n",
      "test epoch: 149 [100 / 100] \n",
      "loss: 0.36855271123349664 | acc: 89.25\n",
      "best test acc: 89.25\n",
      "train epoch: 150 [391 / 391] \n",
      "loss: 0.16589036492435524 | acc: 94.186\n",
      "test epoch: 150 [100 / 100] \n",
      "loss: 0.4495419827103615 | acc: 86.71\n",
      "best test acc: 89.25\n",
      "train epoch: 151 [391 / 391] \n",
      "loss: 0.16326883184673535 | acc: 94.288\n",
      "test epoch: 151 [100 / 100] \n",
      "loss: 0.42416717812418936 | acc: 87.66\n",
      "best test acc: 89.25\n",
      "train epoch: 152 [391 / 391] \n",
      "loss: 0.17348005328222613 | acc: 94.0\n",
      "test epoch: 152 [100 / 100] \n",
      "loss: 0.461676023453474 | acc: 86.8\n",
      "best test acc: 89.25\n",
      "train epoch: 153 [391 / 391] \n",
      "loss: 0.16533092829539342 | acc: 94.258\n",
      "test epoch: 153 [100 / 100] \n",
      "loss: 0.40940538510680197 | acc: 87.87\n",
      "best test acc: 89.25\n",
      "train epoch: 154 [391 / 391] \n",
      "loss: 0.16384296660380596 | acc: 94.346\n",
      "test epoch: 154 [100 / 100] \n",
      "loss: 0.46501123972237113 | acc: 87.49\n",
      "best test acc: 89.25\n",
      "train epoch: 155 [391 / 391] \n",
      "loss: 0.16492207359779826 | acc: 94.142\n",
      "test epoch: 155 [100 / 100] \n",
      "loss: 0.3900554297864437 | acc: 88.69\n",
      "best test acc: 89.25\n",
      "train epoch: 156 [391 / 391] \n",
      "loss: 0.1660826010129336 | acc: 94.226\n",
      "test epoch: 156 [100 / 100] \n",
      "loss: 0.4335688604414463 | acc: 86.81\n",
      "best test acc: 89.25\n",
      "train epoch: 157 [391 / 391] \n",
      "loss: 0.16756013110089485 | acc: 94.18\n",
      "test epoch: 157 [100 / 100] \n",
      "loss: 0.38532924324274065 | acc: 88.56\n",
      "best test acc: 89.25\n",
      "train epoch: 158 [391 / 391] \n",
      "loss: 0.16219703361506352 | acc: 94.344\n",
      "test epoch: 158 [100 / 100] \n",
      "loss: 0.39658144421875474 | acc: 88.0\n",
      "best test acc: 89.25\n",
      "train epoch: 159 [391 / 391] \n",
      "loss: 0.16709095518797865 | acc: 94.258\n",
      "test epoch: 159 [100 / 100] \n",
      "loss: 0.4862123864889145 | acc: 85.72\n",
      "best test acc: 89.25\n",
      "train epoch: 160 [391 / 391] \n",
      "loss: 0.16853851833573694 | acc: 94.126\n",
      "test epoch: 160 [100 / 100] \n",
      "loss: 0.4538446228206158 | acc: 86.93\n",
      "best test acc: 89.25\n",
      "train epoch: 161 [391 / 391] \n",
      "loss: 0.1661741677719309 | acc: 94.144\n",
      "test epoch: 161 [100 / 100] \n",
      "loss: 0.3901817290484905 | acc: 88.22\n",
      "best test acc: 89.25\n",
      "train epoch: 162 [391 / 391] \n",
      "loss: 0.16651048638936503 | acc: 94.186\n",
      "test epoch: 162 [100 / 100] \n",
      "loss: 0.4972229024767876 | acc: 85.87\n",
      "best test acc: 89.25\n",
      "train epoch: 163 [391 / 391] \n",
      "loss: 0.16674769042855334 | acc: 94.152\n",
      "test epoch: 163 [100 / 100] \n",
      "loss: 0.39366976141929627 | acc: 89.02\n",
      "best test acc: 89.25\n",
      "train epoch: 164 [391 / 391] \n",
      "loss: 0.16420355842203435 | acc: 94.22\n",
      "test epoch: 164 [100 / 100] \n",
      "loss: 0.4192795641720295 | acc: 88.36\n",
      "best test acc: 89.25\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "epoch = 0\n",
    "global_steps = 0\n",
    "\n",
    "while True:\n",
    "    epoch += 1\n",
    "    global_steps = train(epoch, global_steps)\n",
    "    best_acc = test(epoch, best_acc)\n",
    "    print(f'best test acc: {best_acc}')\n",
    "\n",
    "    if global_steps > 64000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 16 elements not 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     global_steps \u001b[39m=\u001b[39m train(epoch, global_steps)\n\u001b[1;32m      8\u001b[0m     best_acc \u001b[39m=\u001b[39m test(epoch, best_acc)\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbest test acc: \u001b[39m\u001b[39m{\u001b[39;00mbest_acc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, global_steps)\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MLStudy/cifar-10/model.py:91\u001b[0m, in \u001b[0;36mResnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(x)\n\u001b[1;32m     90\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_2n(x)\n\u001b[0;32m---> 91\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer_4n(x)\n\u001b[1;32m     92\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_6n(x)\n\u001b[1;32m     93\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavg_pool(x)\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MLStudy/cifar-10/model.py:43\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_sample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     shortcut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdown_sample(x)\n\u001b[1;32m     44\u001b[0m out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m shortcut\n\u001b[1;32m     45\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/MLStudy/cifar-10/model.py:16\u001b[0m, in \u001b[0;36mIdentityPadding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m     \u001b[39m# out = F.pad(x, (0,0,0,0,0,self.add_channels))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m# out = self.pooling(out)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n\u001b[0;32m---> 16\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/mlstudy.py39/lib/python3.9/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 16 elements not 32"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "epoch = 0\n",
    "global_steps = 0\n",
    "\n",
    "while True:\n",
    "    epoch += 1\n",
    "    global_steps = train(epoch, global_steps)\n",
    "    best_acc = test(epoch, best_acc)\n",
    "    print(f'best test acc: {best_acc}')\n",
    "\n",
    "    if global_steps > 64000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlstudy.py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
